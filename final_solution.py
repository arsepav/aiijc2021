# -*- coding: utf-8 -*-
"""final_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171UvOz8tJnJlC902Rc7tOmGLvhIorapw

**Скачивание файлов**
"""

import pandas as pd

!wget https://aiijc.com/api_v2/task/1066/53 -O dffolder.zip
!unzip /content/dffolder.zip

!wget https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/q3OBoq2qpSp32w -O parsed.csv #скачивание данных парсинга с headhunter, trudvsem и superjob

parsed = pd.read_csv('/content/parsed.csv').sample(frac=1)[70000:100000] #загрузка данных парсинга
sample = pd.read_csv('/content/rabotaru_en/data/sample.csv')
test = pd.read_csv('/content/rabotaru_en/data/test.csv')
train = pd.read_csv('/content/rabotaru_en/data/train_public.csv')

"""**Импортирование нужных библиотек**"""

!pip install pymorphy2
!pip install catboost

# Commented out IPython magic to ensure Python compatibility.
import pymorphy2
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
 
nltk.download('stopwords')
 
import os
import re
from nltk.corpus import stopwords
from sklearn.metrics import f1_score
from nltk import SnowballStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from catboost import CatBoostClassifier
# %matplotlib inline

a = train[:5]
a

a.to_csv('kapusta.csv')

"""**Обработка данных**"""

stopwords = stopwords.words('russian') #загрузка стоп-слов русского языка

stopwords

morph = pymorphy2.MorphAnalyzer()

def lemmize(text): #функция лемматизации текста
  words = text.split()
  words_splitted = []
  for i in words:
    words_splitted.extend(i.split('-'))
  res = list()
  for word in words_splitted:
        p = morph.parse(word)[0]
        res.append(p.normal_form)
  return ' '.join(res).strip()

train['custom_position'] = train['custom_position'].str.lower().apply(lemmize) #леммитизация текста в custom_position
test['custom_position'] = test['custom_position'].str.lower().apply(lemmize)

parsed['name'] = parsed['name'].str.lower().apply(lemmize)

def remove_stopwords(text): #функция удаления стоп-слов
  words = text.split()
  new_words = []
  for i in words:
    if i not in stopwords:
      new_words.append(i)

  return ' '.join(new_words)

train['custom_position'] = train['custom_position'].str.lower().apply(remove_stopwords) #удаление стоп-слов
test['custom_position'] = test['custom_position'].str.lower().apply(remove_stopwords)
parsed['name'] = parsed['name'].str.lower().apply(remove_stopwords)

train.head()

test.head()

parsed.head()

"""**Предсказание специальностей**"""

#X, y = list(pd.concat([train['custom_position'], parsed['name']])), list(pd.concat([train['target_prof'], parsed['label']]))
X, y = train['custom_position'], train['target_prof']
X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)

y_train

vectorizer = TfidfVectorizer().fit(X_train)

import pickle
with open('vectorizer.pickle', 'wb') as f:
  pickle.dump(vectorizer, f)

X_tfidf_train = np.array(vectorizer.transform(X_train).todense(), dtype=np.float16)
X_tfidf_val = np.array(vectorizer.transform(X_val).todense(), dtype=np.float16)

def evaluate_model(model, X, y, print_report=False): #функция проверки качества модели
  y_pred = model.predict(X)
  f1 = f1_score(y, y_pred, average='macro')
  print(f'Macro F1 score: {f1}')
  if print_report:
    print(f'Report for model: {str(model)}')
    print(classification_report(y, y_pred, zero_division=0))
  return f1

modelq = RandomForestClassifier(n_estimators = 5, n_jobs = -1).fit(X_tfidf_train, y_train) #обучение модели
evaluate_model(modelq, X_tfidf_train, y_train)
print('Validation')
evaluate_model(modelq, X_tfidf_val, y_val)

with open('/content/drive/MyDrive/npk/model5.pickle', 'wb') as f:
  pickle.dump(modelq, f)

test_tfidf = np.array(vectorizer.transform(test['custom_position']).todense(), dtype=np.float16)

test['target_prof'] = modelq.predict(test_tfidf)

test

"""**Предсказание зарплат**"""

test['salary_from'] = 0
test['salary_to'] = 0

for i in range(len(train)): #обработка регионов
  r = str(train['region_id'].iloc[i])
  if r[0] == '{':
    r = r[1:]
  if r[-1] == '}':
    r = r[:-1]
  r = int(r.split(', ')[0])
  train.loc[i, 'region_id'] = r

for i in range(len(test)):
  r = str(test['region_id'].iloc[i])
  if r[0] == '{':
    r = r[1:]
  if r[-1] == '}':
    r = r[:-1]
  r = int(r.split(', ')[0])
  test.loc[i, 'region_id'] = r

train

regs = []
for i in train['region_id']:
  if i not in regs:
    regs.append(i)
reg_median_salary = []
for i in regs:
  a = train[train['region_id'] == i][train['salary_from'] > 0]['salary_from']
  if len(a) == 0:
    reg_median_salary.append(train[train['salary_from'] > 0]['salary_from'].median())
  else:
    reg_median_salary.append(a.median())
reg_median_salary
max_i = max(reg_median_salary) #разделяем на 40 зарплатных классов регионов
for i in range(len(regs)):
  reg_median_salary[i] = round((reg_median_salary[i]/max_i)*40)
for i in range(len(train)): #присваиваем каждой строке один из зарплатных классов
  train.loc[i, 'region_id_i'] = reg_median_salary[regs.index(train['region_id'].iloc[i])]
train
for i in range(len(test)): #присваиваем каждой строке один из зарплатных классов
  try:
    test.loc[i, 'region_id_i'] = reg_median_salary[regs.index(test['region_id'].iloc[i])]
  except:
    test.loc[i, 'region_id_i'] = 18
train

with open('regs.pickle', 'wb') as f:
  pickle.dump(regs, f)
with open('med_s_reg.pickle', 'wb') as f:
  pickle.dump(reg_median_salary, f)

profs = []
for i in train['target_prof']:
  if i not in profs:
    profs.append(i)
profs_median_salary = []
for i in profs:
  a = train[train['target_prof'] == i][train['salary_from'] > 0]['salary_from']
  if len(a) == 0:
    profs_median_salary.append(train[train['salary_from'] > 0]['salary_from'].median())
  else:
    profs_median_salary.append(a.median())
max_i = max(profs_median_salary) #разделяем на 15 зарплатных классов специализаций
for i in range(len(profs)):
  profs_median_salary[i] = round((profs_median_salary[i]/max_i)*15)
for i in range(len(train)): #присваиваем каждой строке один из зарплатных классов
  train.loc[i, 'sal_class_tp'] = profs_median_salary[profs.index(train['target_prof'].iloc[i])]
train
for i in range(len(test)): #присваиваем каждой строке один из зарплатных классов
  test.loc[i, 'sal_class_tp'] = profs_median_salary[profs.index(test['target_prof'].iloc[i])]
test

X, y = train['custom_position'], train['sal_class_tp']
vectorizer = TfidfVectorizer().fit(X_train)
X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)
X_tfidf_train = np.array(vectorizer.transform(X_train).todense(), dtype=np.float16)
X_tfidf_val = np.array(vectorizer.transform(X_val).todense(), dtype=np.float16)
model = RandomForestClassifier(n_estimators = 35, n_jobs = -1, max_depth = 250).fit(X_tfidf_train, y_train) #обучение модели
evaluate_model(model, X_tfidf_train, y_train)
print('Validation')
evaluate_model(model, X_tfidf_val, y_val)

import pickle
with open('model2.pickle', 'wb') as f:
  pickle.dump(model, f)

test_tfidf = np.array(vectorizer.transform(test['custom_position']).todense(), dtype=np.float16)
test['sal_class_tp'] = model.predict(test_tfidf)

import re #функции, которые ищут в описании зарплату
def clear(s):
  d = list(map(str, range(10)))
  m1 = []
  for i in s:
    if i != ' ':
      m1.append(i)
  s = ''.join(m1)
  m = []
  for i in s:
    if i in d:
      m.append(i)
    else:
      m.append(' ')
  s2 = ''.join(m)
  s2 = re.sub(" +", " ", s2)
  return s2

def find_salary(text):
  if False:
    return (-1, -1)
  m = []
  for i in clear(text).split(' '):
    if i != '':
      if 400000 > int(i) > 10000 and (int(i) % 10) == 0:
        m.append(int(i))
  if m == []:
    return (-1, -1)
  if min(m) != max(m):
    return (min(m), max(m))
  return (min(m), -1)

test['salary_from'] = 0 #поиск зарплат в описании
for i in range(len(test)):
  a = find_salary(test['description'].iloc[i])
  test.loc[i, 'salary_from'] = a[0]
  test.loc[i, 'salary_to'] = a[1]
test

columns = ['sal_class_tp','region_id_i', 'offer_experience_year_count', 'offer_education_id', 'operating_schedule_id', 'salary_from', 'salary_to'] #признаки, которые будут использоваться для предсказания зарплат

#датасет для предсказания зарплат
new_train = train[columns]
new_train

new_train.info()

new_train

new_train = new_train[new_train['region_id_i'].notna()]
new_train = new_train[new_train['salary_to'].notna()]
new_train = new_train[new_train['salary_from'].notna()]
new_train = new_train[new_train['offer_experience_year_count'].notna()]

new_train

new_train = new_train.drop(new_train[new_train.salary_from < 1].index)
new_train = new_train.drop(new_train[new_train.salary_to < 1].index)
new_train = new_train.drop(new_train[new_train.region_id_i < 0].index)
new_train = new_train.drop(new_train[new_train.offer_experience_year_count < 0].index)

new_train #обработанный датасет



X_train1, X_val1, y_train1, y_val1 = train_test_split(new_train[['sal_class_tp','region_id_i', 'offer_experience_year_count', 'offer_education_id', 'operating_schedule_id']], new_train['salary_to'], train_size=0.90) #разбиваем на тренировочную и тестовую выборку
X_train2, X_val2, y_train2, y_val2 = train_test_split(new_train[['sal_class_tp','region_id_i', 'offer_experience_year_count', 'offer_education_id', 'operating_schedule_id']], new_train['salary_from'], train_size=0.90)

X_train1

from catboost import CatBoostRegressor

model_to = CatBoostRegressor() #обучаем модели

model_to.fit(X_train1, y_train1)
model_from = CatBoostRegressor()

model_from.fit(X_train2, y_train2)

def MAPE(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

MAPE(y_val1, model_to.predict(X_val1)) #оцениваем точность модели

MAPE(y_val2, model_from.predict(X_val2)) #оцениваем точность модели

to_pred = test[columns]
to_pred

to_pred = to_pred.fillna(0)
to_pred[to_pred['offer_experience_year_count'] < 0] = 0



to_pred

test['salary_form_cat'] = model_from.predict(to_pred.drop(['salary_from',	'salary_to'], axis = 1))
test['salary_to_cat'] = model_to.predict(to_pred.drop(['salary_from',	'salary_to'], axis = 1))

import pickle
with open('cat_to.pickle', 'wb') as f:
  pickle.dump(model_to, f)
with open('cat_from.pickle', 'wb') as f:
  pickle.dump(model_to, f)

for i in range(len(test)):
  if not test['salary_from'].iloc[i] > 0:
    test.loc[i, 'salary_from'] = test['salary_form_cat'].iloc[i]
  if not test['salary_to'].iloc[i] > 0:
    test.loc[i, 'salary_to'] = test['salary_to_cat'].iloc[i]

test[test['salary_from'] != test['salary_form_cat']]

test

result = pd.DataFrame()
result['id'] = test['id']
result['salary_from'] = test['salary_from']
result['salary_to'] = test['salary_to']
result['target_prof'] = test['target_prof']
result

result.to_csv('result12.csv')

def pred(s):
  s = np.array(vectorizer.transform([s]).todense(), dtype=np.float16)
  return modelq.predict(s)[0]

pred('водитель2')

